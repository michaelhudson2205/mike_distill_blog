[
  {
    "path": "posts/2021-12-31-mixed-bag-of-statistical-inference/",
    "title": "Mixed Bag of Statistical Inference",
    "description": "Solved problems with R using statistical inference.",
    "author": [
      {
        "name": "Mike Hudson",
        "url": "https://mike-distill-blog.netlify.app/"
      }
    ],
    "date": "2021-12-31",
    "categories": [
      "statistical inference",
      "t-tests",
      "one-way anova",
      "chi-square",
      "correlation",
      "regression",
      "reporting results"
    ],
    "contents": "\n\n\n\nRelevance to Data Analytics Skills\nKnowledge of statistical inference\nAbility to apply relevant statistical applications to solve problems\nDemonstrated skill in reporting results\nTo the project…\nI wish to show how various problems are solved with R using statistical inference.\nOne Sample t-test The one sample t-test checks to see if there is a difference between one quantitative variable that you collect from your sample, and a standard or recommended value. A one-sample t-test uses data on a single variable in a sample to compare with a specific hypothesized value, whicg is usually based on theory, logic, and/or existing research. A two-tailed single-sample t-test assesses whether the sample mean is significantly different from the comparison value specified while a one-tailed test proposes a specific direction for the hypothesized relationship.\nSome examples of research questions where you might use the one sample t-test include:\nYou are growing tomatoes from seeds. On the back of the seed packet, it says that from the time you plant each seed, it should take 62 days until your first tomato is ready. Out of the 12 seeds you planted, the average time-to-tomato was 65 days. Did it take your plants longer to mature?\nReferences\nGillespie, B.J., Hibbert, K.C., & Wagner III, W.E. 2021 A Guide to R for Social and Behavioral Science Statistics, SAGE.\nRadziwill, N.M. 2019 Statistics (The Easier Way) With R 3ed, Lapis Lucera.\n\n\n\n",
    "preview": "posts/2021-12-31-mixed-bag-of-statistical-inference/MixedBag_stat_infer_1200x800.jpg",
    "last_modified": "2022-02-13T20:52:30+11:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-12-26-titanic-dataset-analysis-part-1/",
    "title": "Titanic Dataset Analysis - Part 1",
    "description": "These common projects can hurt you...",
    "author": [
      {
        "name": "Mike Hudson",
        "url": "https://mike-distill-blog.netlify.app/"
      }
    ],
    "date": "2021-12-26",
    "categories": [],
    "contents": "\n\n\n\nRelevance to Data Analytics Skills\nUsing RMarkdown\nFormatting tables with gt package\n\nUsing R code\nData wrangling\nDealing with missing values\nImputation\n\nKDnuggets has this to say about projects ideas not to include in a data science portfolio.\n\nIt’s suggested not to have common projects in your portfolio. You need to stay away from the most common project ideas when building a portfolio. Try to come up with something that will truly set you apart from the others.\nHere are a few most common projects that can hurt you if you include them in your data science portfolio:\n\n\nSurvival classification on the Titanic dataset.\nDigit classification on the MNIST dataset.\nFlower species classification using the iris dataset.\nThese are the most common projects that can hurt you more than they help you. You can’t find ways to distinguish yourself from others using these datasets. You have to make sure to list novel projects to stand out from the rest.\n\nWell, I am not so sure about that although I accept the point that is being made. The Titanic dataset is so ubiquitous in tutorials for data exploration and, later on, for developing an understanding of the principles of machine learning, it seems that for sure, everybody in data science has seen it or at least heard about it, and has probably cut their data analytics teeth on it themselves. Nevertheless, it is such a rich resource for learning that I felt compelled to include it in my blog, as a multipart series. I hope you find it interesting and if you’ve seen it all before, my apologies.\nPreamble\nInformation drawn from Kaggle :\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\nOn April 15, 1912, during her maiden voyage, the widely considered “unsinkable” RMS Titanic sank after colliding with an iceberg. Unfortunately, there were not enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\nIn Kaggle’s Titanic Machine Learning Competition, participants are asked to build a predictive model that answers the question: “what sorts of people were more likely to survive?” using passenger data (i.e., name, age, gender socio-economic class, etc.). Participants gain access to two similar datasets that include the passenger information. One dataset is titled “train.csv” and the other is titled test.csv”. The “train.csv” dataset contains details of a subset of 891 passengers on board and importantly, reveals whether they survived or not, also known as the “ground truth”. The “test.csv” dataset contains similar information but does not disclose the “ground truth” for each passenger. Using the patterns found in the train.csv data, participants seek to predict whether the other subset of 418 passengers on board, found in test.csv, survived.\nIn this blog post I do some data exploration of the train.csv dataset.\nTo the project…\nFirst thing to do is load the data and take a preliminary look at it. Step 1, load the required R packages.\n\n\n\nStep 2, load the data.\n\n\n\nThere are various functions from various packages that help us get to know the data. The first one I like to use is glimpse().\n\nRows: 891\nColumns: 12\n$ PassengerId <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1…\n$ Survived    <dbl> 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, …\n$ Pclass      <dbl> 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, …\n$ Name        <chr> \"Braund, Mr. Owen Harris\", \"Cumings, Mrs. John B…\n$ Sex         <chr> \"male\", \"female\", \"female\", \"female\", \"male\", \"m…\n$ Age         <dbl> 22, 38, 26, 35, 35, NA, 54, 2, 27, 14, 4, 58, 20…\n$ SibSp       <dbl> 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, …\n$ Parch       <dbl> 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, …\n$ Ticket      <chr> \"A/5 21171\", \"PC 17599\", \"STON/O2. 3101282\", \"11…\n$ Fare        <dbl> 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583…\n$ Cabin       <chr> NA, \"C85\", NA, \"C123\", NA, NA, \"E46\", NA, NA, NA…\n$ Embarked    <chr> \"S\", \"C\", \"S\", \"S\", \"S\", \"Q\", \"S\", \"S\", \"S\", \"C\"…\n\nFortunately, Kaggle provides a Data Dictionary to which I have added some extra details.\n\n\nVariable\n      Description\n      Details\n    PassengerId\nUnique ID for purpose of dataset analysis\nNumericSurvived\nIndicates if the passenger survived the sinking or not\nNumeric [0 1] 0 = no 1 = yesPclass\nPassenger ticket class\nNumeric [1 2 3] 1 = 1st class  2 = 2nd class  3 = 3rd className\nPassenger name\nCharacter [string]Sex\nPassenger sex\nCharacter [female  male]Age\nPassenger age\nNumeric [years]SibSp\nIndicates the number of siblings and/or a spouse also aboard\nNumeric [count]Parch\nIndicates the number of parents and/or children also aboard\nNumeric [count]Ticket\nTicket number\nCharacter [string]Fare\nTicket fare paid\nNumeric [£]Cabin\nCabin number\nCharacter [string]Embarked\nPort of Embarkation\nCharacter [C = Cherbourg  Q = Queenstown  S = Southhampton)\n\nNotes to the variables pclass: A proxy for socio_economic status (SES) 1st = Upper 2nd = Middle 3rd = Lower\nage: Age is fractional if less than 1. If the age is estimated, it is in the form of xx.5\nsibsp: The dataset defines family relations in this way… Sibling = brother, sister, stepbrother, stepsister Spouse = husband, wife (mistresses and fiancés were ignored)\nparch: The dataset defines family relations in this way… Parent = mother, father Child = daughter, son, stepdaughter, stepson Some children travelled only with a nanny, therefore parch = 0 for them.\nThere are 891 rows (observations) and 12 columns (variables). Variables Survived, Pclass, Sex, and Embarked represent factor levels but are recorded as either numerical or character values. These variables will be converted to factor. We can also see missing values (NA) in the variables Cabin, Age and Embarked. We will need to investigate the extent of the missing data.\nThe summary function will help identify how many NAs are in the dataset, at least for the numeric variables, like Age.\n\n  PassengerId       Survived          Pclass          Name          \n Min.   :  1.0   Min.   :0.0000   Min.   :1.000   Length:891        \n 1st Qu.:223.5   1st Qu.:0.0000   1st Qu.:2.000   Class :character  \n Median :446.0   Median :0.0000   Median :3.000   Mode  :character  \n Mean   :446.0   Mean   :0.3838   Mean   :2.309                     \n 3rd Qu.:668.5   3rd Qu.:1.0000   3rd Qu.:3.000                     \n Max.   :891.0   Max.   :1.0000   Max.   :3.000                     \n                                                                    \n     Sex                 Age            SibSp           Parch       \n Length:891         Min.   : 0.42   Min.   :0.000   Min.   :0.0000  \n Class :character   1st Qu.:20.12   1st Qu.:0.000   1st Qu.:0.0000  \n Mode  :character   Median :28.00   Median :0.000   Median :0.0000  \n                    Mean   :29.70   Mean   :0.523   Mean   :0.3816  \n                    3rd Qu.:38.00   3rd Qu.:1.000   3rd Qu.:0.0000  \n                    Max.   :80.00   Max.   :8.000   Max.   :6.0000  \n                    NA's   :177                                     \n    Ticket               Fare           Cabin          \n Length:891         Min.   :  0.00   Length:891        \n Class :character   1st Qu.:  7.91   Class :character  \n Mode  :character   Median : 14.45   Mode  :character  \n                    Mean   : 32.20                     \n                    3rd Qu.: 31.00                     \n                    Max.   :512.33                     \n                                                       \n   Embarked        \n Length:891        \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n                   \n\nWe can see that 177 rows are missing data for Age. We can find the number of NAs in each column as follows\n\nPassengerId    Survived      Pclass        Name         Sex \n          0           0           0           0           0 \n        Age       SibSp       Parch      Ticket        Fare \n        177           0           0           0           0 \n      Cabin    Embarked \n        687           2 \n\nAnother alternative is to use the missmap function from the Amelia package.\n\n\n\nMissing Values\nFrom what I could learn online, there are basically three ways to handle missing values:\n1. Deleting the observations Deletion can be performed in two ways: List Wise Deletion and Pair Wise Deletion.\nIn list wise deletion, we delete observations where any of the variables are missing. For simplicity we can say that this method delets the whole row of observations in which the dating is missing. Simplicity is one of the major advantages of this method, but this method reduces the power of the model because it reduces the sample size. We can use the following code to see how many cases would be left if we were to use list wise deletion.\n\n[1] 183\n\n183 out of 891, or a mere 20.5% of the dataset.\nIn pair wise deletion, we perform analysis with all cases in which the variables of interest are present. Advantage of this method is, it keeps as many cases available for analysis. One of the disadvantages of this method, it uses different sample size for different variables.\nGiven the small size of the dataset, we probably should not opt for deleting either entire observations (rows) or variables (columns) containing missing values.\n2. Mean/Mode/Median (Sensible Value) Imputation Imputation is a method to fill in the missing values with estimated ones. The objective is to employ known relationships that can be identified in the valid values of the date set to assist in estimating the missing values. Mean/Mode/Median imputation is one of the most frequently used methods. It consists of replacing the missing data for a given attribute by the mean or median (quantitative attribute) or mode (qualitative attribute) of all known values of that variable. It can be of two types:\nGeneralised Imputation: In this case, we calculate the mean or median for all non missing values of that variable then replace missing value with mean or median. For example, the mean of Age is calculated as:\n\n[1] 29.69912\n\nSimilar Case Imputation: In this case, we calculate average based on particular attributes. For example, in the current case instead of calculating the overall mean for age we could base the calculation on sex, as follows:\n\n# A tibble: 2 × 2\n  Sex    mean_age\n  <chr>     <dbl>\n1 female     27.9\n2 male       30.7\n\n3. Prediction Prediction models are sophisticated methods for handling missing data. Modeling techniques based on regression, ANOVA, logistic regression, k-nearest neighour, random forest algorithms and so on, are used. R has various packages to deal with missing data, such as:\nmice\nAmelia\nmissForest\nHmisc\nmi\nDMwR\nrpart\nUsing Sensible Value Imputation on missing data for Embarkation Lets grab the two observations that are missing the Embarkment values.\n\n# A tibble: 2 × 12\n  PassengerId Survived Pclass Name      Sex     Age SibSp Parch Ticket\n        <dbl>    <dbl>  <dbl> <chr>     <chr> <dbl> <dbl> <dbl> <chr> \n1          62        1      1 Icard, M… fema…    38     0     0 113572\n2         830        1      1 Stone, M… fema…    62     0     0 113572\n# … with 3 more variables: Fare <dbl>, Cabin <chr>, Embarked <chr>\n\nLets have a look at what the average fare was for females travelling first class and embarking from the three different ports:\n\n# A tibble: 10 × 5\n# Groups:   Embarked [4]\n   Embarked Pclass number mean_fare median_fare\n   <chr>     <dbl>  <int>     <dbl>       <dbl>\n 1 C             1     43     116.        83.2 \n 2 C             2      7      25.3       24   \n 3 C             3     23      14.7       14.5 \n 4 Q             1      1      90         90   \n 5 Q             2      2      12.4       12.4 \n 6 Q             3     33      10.3        7.75\n 7 S             1     48      99.0       79.6 \n 8 S             2     67      21.9       23   \n 9 S             3     88      18.7       14.4 \n10 <NA>          1      2      80         80   \n\nArunkumar Venkataramanan on Kaggle concluded that they most likely embarked from Cherbourg but unfortunately does not give any details as to why he came to that conclusion. It seems to me, looking at the mean and median for 1st class fares for females in the summary above that it was more likely that they embarked from Southhampton. I replace the missing values accordingly:\n\n\n\nLet’s have another look to see how many NAs are in the columns.\n\nPassengerId    Survived      Pclass        Name         Sex \n          0           0           0           0           0 \n        Age       SibSp       Parch      Ticket        Fare \n        177           0           0           0           0 \n      Cabin    Embarked \n        687           0 \n\nOk, 687 for Cabin, and 177 for Age. Cabin is a tough one (for me). I’m going to leave dealing with that variable until a future post where I get the data ready for machine learning. At this stage I just want to do some preliminary exploration on the data so I will, for the time being, see how I can deal with the missing Age values. First up, I will insert two new columns that are copies of the Age column. I want to use two different predictive imputation packages (rpart and mice) to substitute values for the missing Age values and I want the replicated columns so I can compare the original values with the new.\n\n\n\nYou can get information about the rpart package here.\n\n\nFALSE  TRUE \n  714   177 \n\n\n\n\n\n\nFALSE \n  891 \n\nYou can get information the mice package here.\n\n\nFALSE  TRUE \n  714   177 \n\n\n\n iter imp variable\n  1   1  age_mice\n  1   2  age_mice\n  1   3  age_mice\n  1   4  age_mice\n  1   5  age_mice\n  2   1  age_mice\n  2   2  age_mice\n  2   3  age_mice\n  2   4  age_mice\n  2   5  age_mice\n  3   1  age_mice\n  3   2  age_mice\n  3   3  age_mice\n  3   4  age_mice\n  3   5  age_mice\n  4   1  age_mice\n  4   2  age_mice\n  4   3  age_mice\n  4   4  age_mice\n  4   5  age_mice\n  5   1  age_mice\n  5   2  age_mice\n  5   3  age_mice\n  5   4  age_mice\n  5   5  age_mice\n\n\n\nFALSE \n  891 \n\nCompare the different imputations for age:\n\n# A tibble: 2 × 7\n  Sex    mean_age sd_age mean_rpart sd_rpart mean_mice sd_mice\n  <chr>     <dbl>  <dbl>      <dbl>    <dbl>     <dbl>   <dbl>\n1 female     27.9   14.1       27.7     13.3      28      14.3\n2 male       30.7   14.7       30.5     13.5      30.6    14.3\n\nInteresting. I will use age_rpart for the rest of this blog post.\nAs previously discussed the variables Survived, Pclass, Sex, and Embarked are better treated as factors:\n\n\n\nSurvived Count\n\n\n\nSurvived Count by Sex\n\n\n\nSurvival by Pclass\n\n\n\nAge Density\n\n\n\nSurvival by Age\nFirst cut the age_rpart variable into bins of 10 year spans:\n\n\n\nThen plot:\n\n\n\nResources\nhttps://www.kdnuggets.com/2021/10/data-science-portfolio-project-ideas.html\nhttps://www.kaggle.com/c/titanic/overview\nhttps://medium.com/swlh/basic-exploratory-data-analysis-of-titanic-data-using-r-53d4b764ec89\nhttp://r-statistics.co/Missing-Value-Treatment-With-R.html\nhttps://medium.com/coinmonks/dealing-with-missing-data-using-r-3ae428da2d17\nhttps://www.kaggle.com/arunkumarramanan/data-science-in-r-and-titanic-survival-prediction/notebook\nhttps://rstudio-pubs-static.s3.amazonaws.com/602920_08b3060ff9544f5e97ae4ed70c95d491.html\n\n\n\n",
    "preview": "posts/2021-12-26-titanic-dataset-analysis-part-1/titanic_part_1.png",
    "last_modified": "2021-12-30T19:43:19+11:00",
    "input_file": {},
    "preview_width": 1200,
    "preview_height": 800
  },
  {
    "path": "posts/2021-12-19-creating-a-leaflet-map-in-r/",
    "title": "Creating a leaflet map in R",
    "description": "Using an API to access geospatial data.",
    "author": [
      {
        "name": "Mike Hudson",
        "url": "https://mike-distill-blog.netlify.app/"
      }
    ],
    "date": "2021-12-19",
    "categories": [
      "API",
      "Google Maps",
      "Leaflet",
      "htmlwidgets"
    ],
    "contents": "\n\n\n\nRelevance to Data Analytics Skills\nUsing RMarkdown\nAccessing data from Google Maps API\nData wrangling\nUsing R code\nCreating interactive map with Leaflet\nProviding data in form that provides value to users\nAbility to find resources & help online\nTo the project…\nAt a recent interview I had several questions about APIs, so I thought I would do a few projects that involve retrieving data from online applications’ databases. API is the acronym for Application Programming Interface, which is a software intermediary that allows two applications to talk to each other.\nIn this project I set up an account with the Google Maps Platform which then allowed me to serve up a list of street addresses and receive back their latitude and longitude coordinates. I needed these in order to place the markers on the leaflet map I was using for the visualisation. I started with an Excel spreadsheet as shown below:\n\n\n\nI saved that file as a .csv and then brought it into R to perform the following:\n\n\n\nThe ggmap::geocode() returns the latitude and longitude for street addresses after accessing the Google Maps Platform API.\nI did some more wrangling of the hasi_coords.csv file to produce the following:\n\n\n\nNow, with the data ready, I proceeded to make the map.\n\n\nlibrary(tidyverse)\nlibrary(leaflet)\nlibrary(leaflet.extras)\n\nhasi_coords <- read_csv(\"hasi_coords_A.csv\")\n\nhasi_coords$file <- as.character(hasi_coords$file)\n\nAdam_data <- hasi_coords %>% \n  filter(Keyworker == \"Adam\")\n\nAllen_data <- hasi_coords %>% \n  filter(Keyworker == \"Allen\")\n\nCarrie_data <- hasi_coords %>% \n  filter(Keyworker == \"Carrie\")\n\nEbony_data <- hasi_coords %>% \n  filter(Keyworker == \"Ebony\")\n\nGordon_data <- hasi_coords %>% \n  filter(Keyworker == \"Gordon\")\n\nJen_data <- hasi_coords %>% \n  filter(Keyworker == \"Jen\")\n\nKalindi_data <- hasi_coords %>% \n  filter(Keyworker == \"Kalindi\")\n\nKerryM_data <- hasi_coords %>% \n  filter(Keyworker == \"KerryM\")\n\nMike_data <- hasi_coords %>% \n  filter(Keyworker == \"Mike\")\n\npal <- colorFactor(palette = c(\"olivedrab\", \"goldenrod\"),\n                   levels = c(\"Nowra\", \"Ulladulla\"))\n\nAdam_icons <- awesomeIcons(\n  icon = \"ios-close\",\n  iconColor = \"black\",\n  library = \"ion\",\n  markerColor = Adam_data$site_colour\n)\n\nAllen_icons <- awesomeIcons(\n  icon = \"ios-close\",\n  iconColor = \"black\",\n  library = \"ion\",\n  markerColor = Allen_data$site_colour\n)\n\nCarrie_icons <- awesomeIcons(\n  icon = \"ios-close\",\n  iconColor = \"black\",\n  library = \"ion\",\n  markerColor = Carrie_data$site_colour\n)\n\nEbony_icons <- awesomeIcons(\n  icon = \"ios-close\",\n  iconColor = \"black\",\n  library = \"ion\",\n  markerColor = Ebony_data$site_colour\n)\n\nGordon_icons <- awesomeIcons(\n  icon = \"ios-close\",\n  iconColor = \"black\",\n  library = \"ion\",\n  markerColor = Gordon_data$site_colour\n)\n\nJen_icons <- awesomeIcons(\n  icon = \"ios-close\",\n  iconColor = \"black\",\n  library = \"ion\",\n  markerColor = Jen_data$site_colour\n)\n\nKalindi_icons <- awesomeIcons(\n  icon = \"ios-close\",\n  iconColor = \"black\",\n  library = \"ion\",\n  markerColor = Kalindi_data$site_colour\n)\n\nKerryM_icons <- awesomeIcons(\n  icon = \"ios-close\",\n  iconColor = \"black\",\n  library = \"ion\",\n  markerColor = KerryM_data$site_colour\n)\n\nMike_icons <- awesomeIcons(\n  icon = \"ios-close\",\n  iconColor = \"black\",\n  library = \"ion\",\n  markerColor = Mike_data$site_colour\n)\n\nm1 <- leaflet(width = \"100%\") %>% \n  addTiles(group = \"OSM\") %>% \n  addProviderTiles(\"CartoDB\", group = \"Carto\") %>% \n  addProviderTiles(\"Esri\", group = \"Esri\") %>% \n  addAwesomeMarkers(data = Adam_data, icon = Adam_icons, label = ~paste0(Keyworker,\" \",file,\" \",name), group = \"Adam\") %>%\n  addAwesomeMarkers(data = Allen_data, icon = Allen_icons, label = ~paste0(Keyworker,\" \",file,\" \",name), group = \"Allen\") %>%\n  addAwesomeMarkers(data = Carrie_data, icon = Carrie_icons, label = ~paste0(Keyworker,\" \",file,\" \",name), group = \"Carrie\") %>%\n  addAwesomeMarkers(data = Ebony_data, icon = Ebony_icons, label = ~paste0(Keyworker,\" \",file,\" \",name), group = \"Ebony\") %>%\n  addAwesomeMarkers(data = Gordon_data, icon = Gordon_icons, label = ~paste0(Keyworker,\" \",file,\" \",name), group = \"Gordon\") %>%\n  addAwesomeMarkers(data = Jen_data, icon = Jen_icons, label = ~paste0(Keyworker,\" \",file,\" \",name), group = \"Jen\") %>%\n  addAwesomeMarkers(data = Kalindi_data, icon = Kalindi_icons, label = ~paste0(Keyworker,\" \",file,\" \",name), group = \"Kalindi\") %>%\n  addAwesomeMarkers(data = KerryM_data, icon = KerryM_icons, label = ~paste0(Keyworker,\" \",file,\" \",name), group = \"KerryM\") %>%\n  addAwesomeMarkers(data = Mike_data, icon = Mike_icons, label = ~paste0(Keyworker,\" \",file,\" \",name), group = \"Mike\") %>%\n  \n  addLayersControl(baseGroups = c(\"OSM\", \"Carto\", \"Esri\"),\n                   overlayGroups = c(\"Adam\",\"Allen\",\"Carrie\",\"Ebony\",\"Gordon\",\"Jen\",\"Kalindi\",\"KerryM\",\"Mike\")) %>% \n  addSearchFeatures(\n    targetGroups = c(\"Adam\",\"Allen\",\"Carrie\",\"Ebony\",\"Gordon\",\"Jen\",\"Kalindi\",\"KerryM\",\"Mike\"),\n    options = searchFeaturesOptions(zoom = 20)\n  ) %>%\n  addResetMapButton() %>% \n  addFullscreenControl() %>% \n  addLegend(position = \"bottomright\",\n            pal = pal,\n            values = c(\"Nowra\",\"Ulladulla\"))\n\nm1\n\n\n\n\nInteractive Map\nPlease have a play around with the interactive map rendered above. The controls in the top-left corner are Zoom in, Zoom out, Fullscreen, Search, and Reset View. You can zoom in and out with your mouse wheel, and left-click and hold then drag the map around to pan. The control in the top-right is the layer control and from there you can select different base maps, and select different Keyworkers to show/hide the markers. The search function lets the user search for any text contained in the labels that display when you hover over a marker. Of course, its hard to use the search function when you don’t know what data is available but just try typing in different letters or numbers and you’ll see how it produces selections for you. Click on any entry for the map to focus on that particular marker. Go fullscreen, zoom in, zoom out, pan, have fun. Think of ways an interactive map could be of use to you and send me your ideas.\nWrap-up\nThis project took a lot of googling and reading stackoverflow posts to get leaflet behaving the way I wanted. It took me a while to work out that separate layers needed to be created for each Keyworker in order to get the overlayGroups control and the Seach function working properly, and I had quite a challenge to get the markers to display different colours according to which site the client belonged to. Also, labels don’t behave the same way as popups so there was a lot of mucking around with getting this sorted. I’ll list the websites and stackoverflow posts I found helpful below. If you’ve read this far, I thank you for your time and interest in looking at my blog. :-) Mike\nHelpful resources:\nDataCamp: Interactive Maps with leaflet in R\nLeaflet\nLeaflet for R: Markers\nLeaflet for R: Show/Hide Layers\nColorBrewer 2.0\nstackoverflow: Multiple markers on same coordinate\nstackoverflow: Adjust size of leaflet map in rmarkdown html\nstackoverflow: Change color of leaflet marker Finding the R.framework directory on a Mac\n\n\n\n",
    "preview": "posts/2021-12-19-creating-a-leaflet-map-in-r/blog_image_1200x636.jpg",
    "last_modified": "2021-12-19T22:13:43+11:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-12-10-poker-hands-probabilities/",
    "title": "Poker Hands Probabilities",
    "description": "Poker as a statistical experiment.",
    "author": [
      {
        "name": "Mike Hudson",
        "url": "https://mike-distill-blog.netlify.app/"
      }
    ],
    "date": "2021-12-10",
    "categories": [
      "probability",
      "html",
      "LaTeX"
    ],
    "contents": "\n\n\n\nFigure 1: Source: https://www.poker.org/wp-content/uploads/2020/12/Poker_Info_Best_Poker_Hands_1-scaled.jpg\n\n\n\nRelevance to Data Analytics Skills\nUsing RMarkdown\nUsing LaTeX in RMarkdown\nUsing HTML in RMarkdown\nUsing R code\nAbility to explain statistical concepts\nAbility to explain other people’s code\nAbility to find resources online\nTo the project…\nAn interesting exercise is to use R to explain and analyse the probabilities of different hands in draw poker. In this exercise, a hand of poker consists of five cards drawn randomly without replacement from a single, well-shuffled deck of 52 cards. The total number of possible five card hands, drawn from a 52 card deck without replacement and given that order does not matter, is:\n\\[\\begin{align*}\nTotal\\ number\\ of\\ possible\\ poker\\ hands = {_{52}C_5} & = \\frac{52!}{5! \\times 47!} \\\\\n\\\\\n& = \\frac{52 \\times 51 \\times 50 \\times 49 \\times 48}{5 \\times 4 \\times 3 \\times 2 \\times 1} \\\\\n\\\\\n& = 2,598,960\n\\end{align*}\\]\n\nThe poker hands of interest are shown in the image at the top of this post and are described as:\n\nRank\n\n\nName\n\n\nDescription\n\n\n1\n\n\nRoyal Flush\n\n\nThe hand contains the A, K, Q, J and 10 of the same suit.\n\n\n2\n\n\nStraight Flush\n\n\nThe hand contains five cards of the same suit with consecutive values. A can come before a 2, but not after K (as the hand would then be a Royal Flush).\n\n\n3\n\n\nFour of a Kind\n\n\nThe hand contains four cards of the same rank (one for each suit)\n\n\n4\n\n\nFull House\n\n\nThe hand contains three cards of one rank and two cards of a different rank.\n\n\n5\n\n\nFlush\n\n\nThe hand contains five cards of the same suit, but not a Straight Flush.\n\n\n6\n\n\nStraight\n\n\nThe hand contains five cards with consecutive ranks that are not a Straight Flush.\n\n\n7\n\n\nThree of a Kind\n\n\nThe hand contains three cards of the same rank and is not a Full House or Four of a Kind.\n\n\n8\n\n\nTwo Pairs\n\n\nThe hand contains two pairs, each of a different rank.\n\n\n9\n\n\nOne pair\n\n\nThe hand contains two cards of the same rank and is not a Full House or Four of a Kind.\n\n\n10\n\n\nHighest card\n\n\nThe hand is not any of the above.\n\nRoyal Flush and Straight Flush\nThe easiest way to solve this problem is to simply list all of the possible Straight Flush hands (of which the Royal Flush is a subset):\n\n\n\n\n\n\n\n\n\n\n\nA, 2, 3, 4, 5\n\n\n2, 3, 4, 5, 6\n\n\n3, 4, 5, 6, 7\n\n\n4, 5, 6, 7, 8\n\n\n5, 6, 7, 8, 9\n\n\n6, 7, 8, 9, 10\n\n\n7, 8, 9, 10, J\n\n\n8, 9, 10, J, Q\n\n\n9, 10, J, Q, K\n\n\n10, J, Q, K, A\n\nThe last hand shown corresponds to a Royal flush, while the other nine are regular Straight Flushes. Since there are four suits in the deck, there are four combinations of cards that yield a Royal Flush and 36 (9 x 4) that yield a Straight Flush. Therefore:\n\\[\\begin{align*}\nP(Royal\\ Flush) & = \\frac{4}{2,598,960} = 0.00000153908 \\\\\n\\\\\nP(Straight\\ Flush) & = \\frac{36}{2,598,960} = 0.00001385169\n\\end{align*}\\]\nOn average, a Royal Flush is dealt one time in every 649,740 deals, and a Straight Flush is dealt one time in every 72,193 deals.\nFour of a Kind\nThe number of ways that five cards can be dealt to produce Four of a Kind requires three independent choices:\nChoose the rank of the card that appears four times in the hand. A playing card can have a rank of 2, 3, 4, 5, 6, 7, 8, 9, 10, J, Q, K, or A. For Four of a Kind we choose 1 rank from a set of 13 ranks. The number of ways to do this is \\(_{13}C_1 = 13\\).\nChoose one rank for the fifth card. There are 12 remaining ranks to choose from. The number of ways to do this is \\(_{12}C_1 = 12\\).\nChoose a suit for the fifth card. There are four suits to choose from. The number of ways to do this is \\(_4C_1 = 4\\).\nThe number of ways to produce Four of a Kind is equal to the product of the number of ways to make each independent choice. Therefore: \\[Number\\ of\\ Four\\ of\\ a\\ Kind\\ Hands\\ =\\ _{13}C_1 \\times _{12}C_1 \\times _4C_1 = 13 \\times 12 \\times 4 = 624\\]\nIn other words, there are 13 ranks, so the number of possible sets of four cards is 13. The fifth card can be any one of the 48 cards left in the deck.\nFinally, we compute the probability. \\[P(Four\\ of\\ a\\ Kind) = \\frac{624}{2,598,960} = 0.00024009604\\]\nOn average, Four of a Kind is dealt one time in every 4,165 deals.\nFull House\nTo count the number of ways that five cards can be dealt to produce a Full House requires four independent choices:\nChoose the rank of cards in the hand. For a Full House, we choose 2 ranks from a set of 13 ranks. The number of ways to do this is \\(_{13}C_2\\).\nChoose one rank for the three-card combination. There are 2 ranks in a Full House, from which we choose one. The number of ways to do this is \\(_2C_1\\).\nChoose suits for the three-card combination. There are four suits, from which we choose three. The number of ways to do this is \\(_4C_3\\).\nChoose suits for the two-card combination. There are four suits, from which we choose two. The number of ways to do this is \\(_4C_2\\).\nThe number of ways to produce a Full House is equal to the product of the number of ways to make each independent choice. Therefore:\n\\[\\begin{align*}\nNumber\\ of\\ Full\\ House\\ hands & = _{13}C_2 \\times _2C_1 \\times _4C_3 \\times _4C_2 \\\\\n\\\\\n& = 78 \\times 2 \\times 4 \\times 6 \\\\\n\\\\\n& = 3,744\n\\end{align*}\\]\nOr to put it another way, for the number of three-card combinations, we have 13 possible options for the rank, and 4 options for the combination of suits associated with these three cards (that is \\(_4C_3 = 4\\)). Therefore, the number of three-card combinations is 13 x 4 = 52. There are now 12 possible ranks that can be used for the two-card combination, and there are \\(_4C_2 = \\frac{4!}{2! \\times 2!} = 6\\) combinations of suits for that rank, for a total of 12 x 6 = 72 distinct two-card combinations. This gives \\(13 \\times 4 \\times 12 \\times 6 = 52 \\times 72 = 3,744\\) unique Full House hands.\nFinally, we compute the probability. \\[P(Full\\ House) = \\frac{3,744}{2,598,960} = 0.00144057623\\]\nBased on this result, a Full House is dealt, on average, approximately one time in every 694 deals.\nFlush\nThe Venn diagram below shows the relationship between Straight Flushes, and what we will call Ordinary Flushes.\nVenn Diagram of Flush handsEverything within the rectangle is a Flush, that is, a poker hand with five cards in the same suit. The blue circle contains all the hands that are an Ordinary Flush, and the red circle contains all the hands that are a Straight Flush. The Ordinary Flush and the Straight Flush are mutually exclusive events. Accordingly, \\(P_f = P_{sf} + P_{of}\\), where \\(P_f\\) is the probability of any type of flush, \\(P_{sf}\\) is the probability of a Straight Flush, \\(P_{of}\\) is the probability of an Ordinary Flush. From the analysis in a previous section, we already know that the number of Straight Flush hands is 40. To count the number of ways that five cards can be dealt to produce any Flush hand (i.e. the number of Flush hands contained in the rectangle) requires two independent choices:\nChoose the rank of each card in the hand. That is, choose five ranks from the set of 13 distinct ranks. The number of ways to do this is \\(_{13}C_5\\).\nChoose one suit for the hand. There are four suits to choose from, so the number of ways to do this is \\(_4C_1\\).\nThe number of ways to produce a Flush is equal to the product of the number of ways to make each independent choice. Therefore:\n\\[\\begin{align*}\nTotal\\ number\\ of\\ Flush\\ hands & = {_{13}C_5} \\times {_4C_1} \\\\\n\\\\\n& = 1,287 \\times 4 \\\\\n\\\\\n& = 5,148\n\\end{align*}\\]\nFinally, we compute the probability for Ordinary Flush hands by subtracting the number of Straight Flush hands from the total for all Flush hands, \\(5,148 - 40 = 5,108\\). We can now find the probability of being dealt an Ordinary Flush: \\[P(Ordinary\\ Flush) = \\frac{5,108}{2,598,960} = 0.001965400155\\]\nBased on this result, an Ordinary Flush is dealt, on average, once every 509 deals.\nStraight\nThe Venn diagram below shows the relationship between Straight Flushes, and what we will call Ordinary Straights.\nVenn Diagram of Straight handsEverything within the rectangle is a Straight, that is, a poker hand with five cards in sequence. The green circle contains all the hands that are an Ordinary Straight, and the red circle contains all the hands that are a Straight Flush. The Ordinary Straight and the Straight Flush are mutually exclusive events. Accordingly, \\(P_s = P_{sf} + P_{os}\\), where \\(P_s\\) is the probability of any type of straight, \\(P_{sf}\\) is the probability of a Straight Flush, \\(P_{os}\\) is the probability of an Ordinary Straight. From the analysis in a previous section, we already know that the number of Straight Flush hands is 40. To count the number of ways that five cards can be dealt to produce any Straight hand (i.e. the number of Straight hands contained in the rectangle) requires six independent choices:\nChoose the rank of the lowest card in the hand. For a straight, the lowest card can be A, 2, 3, 4, 5, 6, 7, 8, 9, 10. So we choose one rank from a set of 10 ranks. The number of ways to do this is \\(_{10}C_1\\).\nChoose one suit for the first card in the hand. There are four suits to choose from, so the number of ways to do this is \\(_4C_1\\).\nChoose one suit for the second card in the hand. There are four suits to choose from, so the number of ways to do this is \\(_4C_1\\).\nChoose one suit for the third card in the hand. There are four suits to choose from, so the number of ways to do this is \\(_4C_1\\).\nChoose one suit for the fourth card in the hand. There are four suits to choose from, so the number of ways to do this is \\(_4C_1\\).\nChoose one suit for the fifth card in the hand. There are four suits to choose from, so the number of ways to do this is \\(_4C_1\\).\nThe number of ways to produce a Straight is equal to the product of the number of ways to make each independent choice. Therefore:\n\\[\\begin{align*}\nTotal\\ number\\ of\\ Straight\\ hands & = {_{10}C_1} \\times {_4C_1} \\times {_4C_1} \\times {_4C_1} \\times {_4C_1} \\times {_4C_1} \\\\\n\\\\\n& = 10 \\times 4 \\times 4 \\times 4 \\times 4 \\times 4 \\\\\n\\\\\n& = 10,240\n\\end{align*}\\]\nFinally, we compute the probability for Ordinary Straight hands by subtracting the number of Straight Flush hands from the total for all Straight hands, \\(10,240 - 40 = 10,200\\). We can now find the probability of being dealt an Ordinary Straight: \\[P(Ordinary\\ Straight) = \\frac{10,200}{2,598,960} = 0.00392464678\\]\nBased on this result, an Ordinary Straight is dealt, on average, once every 255 deals.\nThree of a Kind\nThe same general approach is used as that for the Full House. To count the number of ways that five cards can be dealt to produce three of a kind requires five independent choices:\nChoose the rank for cards of matching rank. For Three of a Kind, we choose 1 rank from a set of 13 ranks. The number of ways to do this is \\(_{13}C_1\\).\nChoose the ranks for the non-matching cards. There are 12 remaining ranks, from which we choose two. The number of ways to do this is \\(_{12}C_2\\).\nChoose suits for the three-card combination. There are four suits, from which we choose three. The number of ways to do this is \\(_4C_3\\).\nChoose a suit for one of the non-matching cards. There are four suits, from which we choose one. The number of ways to do this is \\(_4C_1\\).\nChoose a suit for the other non-matching cards. There are four suits, from which we choose one. The number of ways to do this is \\(_4C_1\\).\nThe number of ways to produce the Three of a Kind hand is equal to the product of the number of ways to make each independent choice. Therefore:\n\\[\\begin{align*}\nNumber\\ of\\ Three\\ of\\ a\\ Kind\\ hands & = {_{13}C_1} \\times {_{12}C_2} \\times {_4C_3} \\times {_4C_1} \\times {_4C_1} \\\\\n\\\\\n& = 13 \\times 66 \\times 4 \\times 4 \\times 4 \\\\\n\\\\\n& = 54,912\n\\end{align*}\\]\nFinally, we compute the probability. \\[P(Three\\ of\\ a\\ Kind) = \\frac{54,912}{2,598,960} = 0.021128455138\\]\nBased on this result, Three of a Kind is dealt, on average, approximately one time in every 47 deals.\nTwo Pairs\nTo count the number of ways that five cards can be dealt to produce two pairs requires five independent choices:\nChoose the rank for cards of matching rank. For Two Pairs, we choose 2 ranks from a set of 13 ranks. The number of ways to do this is \\(_{13}C_2\\).\nChoose the rank of the remaining non-matching card. There are 11 remaining ranks, from which we choose one. The number of ways to do this is \\(_{11}C_1\\).\nChoose suits for the first two-card combination. There are four suits, from which we choose two. The number of ways to do this is \\(_4C_2\\).\nChoose suits for the second two-card combination. There are four suits, from which we choose two. The number of ways to do this is \\(_4C_2\\).\nChoose a suit for the non-matching card. There are four suits, from which we choose one. The number of ways to do this is \\(_4C_1\\).\nThe number of ways to produce Two Pairs is equal to the product of the number of ways to make each independent choice. Therefore:\n\\[\\begin{align*}\nNumber\\ of\\ Two\\ Pairs\\ hands & = {_{13}C_2} \\times {_{11}C_1} \\times {_4C_2} \\times {_4C_2} \\times {_4C_1} \\\\\n\\\\\n& = 78 \\times 11 \\times 6 \\times 6 \\times 4 \\\\\n\\\\\n& = 123,552\n\\end{align*}\\]\nFinally, we compute the probability. \\[P(Two\\ Pairs) = \\frac{123,552}{2,598,960} = 0.04753901561\\]\nBased on this result, Two Pairs is dealt, on average, approximately one time in every 21 deals.\nOne Pair\nTo count the number of ways that five cards can be dealt to produce one pair requires six independent choices:\nChoose the rank for the cards of matching rank. For One Pair, we choose 1 rank from a set of 13 ranks. The number of ways to do this is \\(_{13}C_1\\).\nChoose the rank of the remaining non-matching cards. There are 12 remaining ranks, from which we choose three. The number of ways to do this is \\(_{12}C_3\\).\nChoose suits for the cards of matching rank. There are four suits, from which we choose two. The number of ways to do this is \\(_4C_2\\).\nChoose a suit for the first non-matching card. There are four suits, from which we choose one. The number of ways to do this is \\(_4C_1\\).\nChoose a suit for the second non-matching card. There are four suits, from which we choose one. The number of ways to do this is \\(_4C_1\\).\nChoose a suit for the third non-matching card. There are four suits, from which we choose one. The number of ways to do this is \\(_4C_1\\).\nThe number of ways to produce One Pair is equal to the product of the number of ways to make each independent choice. Therefore:\n\\[\\begin{align*}\nNumber\\ of\\ One\\ Pair\\ hands & = {_{13}C_1} \\times {_{12}C_3} \\times {_4C_2} \\times {_4C_1} \\times {_4C_1} \\times {_4C_1} \\\\\n\\\\\n& = 13 \\times 220 \\times 6 \\times 4 \\times 4 \\times 4 \\\\\n\\\\\n& = 1,098,240\n\\end{align*}\\]\nFinally, we compute the probability. \\[P(One\\ Pair) = \\frac{1,098,240}{2,598,960} = 0.42256902761\\]\nOn any given hand, there is on average a 42% chance of being dealt One Pair.\nSummary Table\n(2,598,960 unique hands from a 52 card deck)\n\nRank\n\n\nName\n\n\nCount\n\n\nProbabilty\n\n\nAvg.Deals\n\n\n1\n\n\nRoyal Flush\n\n\n4\n\n\n0.00000153908\n\n\n649,740\n\n\n2\n\n\nStraight Flush\n\n\n36\n\n\n0.00001385169\n\n\n72,193\n\n\n3\n\n\nFour of a Kind\n\n\n624\n\n\n0.00024009604\n\n\n4165\n\n\n4\n\n\nFull House\n\n\n3,744\n\n\n0.00144057623\n\n\n694\n\n\n5\n\n\nFlush\n\n\n5,108\n\n\n0.00196540155\n\n\n509\n\n\n6\n\n\nStraight\n\n\n10,200\n\n\n0.00392464678\n\n\n255\n\n\n7\n\n\nThree of a Kind\n\n\n54,912\n\n\n0.02112845138\n\n\n47\n\n\n8\n\n\nTwo Pairs\n\n\n123,552\n\n\n0.04753901561\n\n\n21\n\n\n9\n\n\nOne Pair\n\n\n1,098,240\n\n\n0.42256902761\n\n\n2.4\n\nSimulation Using R\nThe following R code is heavily based on the code I found at the Duke University site, link at the end of this blog post. That source code comes without explanation and without any comments. I attempt to rectify that by providing interpretation and making clear what is going on. I am not an R expert so please accept that this is a learning exercise for me, but your feedback and suggestions are welcome!\nFirst, create an object to hold the deck.\nI found the code from Duke Uni a little confusing, so I used the following. It may be verbose but its easier to understand:\n\n\nrank <- rep(c(2:10, \"J\", \"Q\", \"K\", \"A\"), 4)\nsuit <- rep(c(\"C\" ,\"D\" ,\"H\" ,\"S\"), each = 13)\ndeck <- cbind(rank, suit)\n\n\n\nYou can find a good explanation of the rep function at Statistics Globe, and I would recommend that YouTube channel for learning about using R.\nNext, we construct three functions. One for dealing a single hand of five cards, drawn at random from our deck. Another function for determining what kind of hand has been dealt, for example Full House, Straight, etc, or if nothing higher, High Card. The third function is for simulating user determined numbers of deals and returning the number of each kind of hand dealt in that simulation.\n\n\n# a function that returns a random hand of 5 cards drawn from our deck of 52 cards\ndeal_hand <- function(){\n    return(deck[sample(1:52, 5, replace = FALSE),])\n}\n\n\n\n\n\n# a function that determines what kind oh hand has been dealt\nwhat_hand <- function(hand){\n    ranks_acehigh = c(2:10, \"J\", \"Q\", \"K\", \"A\")\n    ranks_acelow = c(\"A\", 2:10, \"J\", \"Q\", \"K\")\n    \n    rank_i_ah = sort(sapply(hand[, \"rank\"], function(x) which(x == ranks_acehigh)))\n    rank_i_al = sort(sapply(hand[, \"rank\"], function(x) which(x == ranks_acelow)))\n    \n    is_straight = all(rank_i_ah - min(rank_i_ah) + 1 == 1:5) | all(rank_i_al - min(rank_i_al) + 1 == 1:5)\n    is_flush = length(unique(hand[, \"suit\"])) == 1\n    \n    if(is_straight && is_flush){\n        if(all(c(\"K\", \"A\") %in% hand[, \"rank\"])) return(\"Royal Flush\")\n        else return(\"Straight Flush\")\n    }\n    \n    if(is_straight) return(\"Straight\")\n    if(is_flush) return(\"Flush\")\n    \n    tab = sort(table(hand[, \"rank\"]))\n    if(length(tab) == 2){\n        if(all(tab == c(1, 4))) return(\"Four of a Kind\")\n        if(all(tab == c(2, 3))) return(\"Full House\")\n    }\n    if(length(tab) == 3){\n        if(all(tab == c(1, 1, 3))) return(\"Three of a Kind\")\n        if(all(tab == c(1, 2, 2))) return(\"Two Pairs\")\n    }\n    if(length(tab) == 4){\n        return(\"One Pair\")\n    }\n    return(\"High Card\")\n}\n\n\n\n\n\n# a function that counts the different kinds of poker hands from a simulation of N number of deals\nsimulate_deals <- function(N = 10000){\n    hands = c(\"Royal Flush\", \"Straight Flush\", \"Four of a Kind\",\n              \"Full House\", \"Flush\", \"Straight\", \"Three of a Kind\",\n              \"Two Pairs\", \"One Pair\", \"High Card\")\n    \n    res = matrix(rep(0, length(hands)), ncol = 1)\n    rownames(res) = hands\n    colnames(res) = \"Counts\"\n    \n    pb = txtProgressBar(min = 0, max = N, style = 3)\n    for(i in 1:N){\n        hand = what_hand(deal_hand())\n        res[hand,1] = res[hand,1] + 1\n        setTxtProgressBar(pb, i)\n    }\n    return(res)\n}\n\n\n\nWrap-up\nThis little project was well worth the effort. Using HTML and LaTeX within RMarkdown takes some doing but knowing how to format and set out tables and formulas is necessary in order to produce a post that entices the reader to spend their time looking it over. Like anything, practice makes perfect, and that’s especially true for RMarkdown and coming to terms with all its idiosynchracies. Also, going over the R code from Duke was tremendously beneficial in learning how to use base R efficiently and how to construct functions that flow logically. If you’ve read this far, I thank you for your time and interest in looking at my blog. :-) Mike\nThis blog post draws from the following resources:\nRodriguez, Abel & Mendes, Bruno. (2018). Probability, Decisions and Games: A Gentle Introduction Using R, John Wiley & Sons.\nStat Trek: Teach yourself statistics\nDuke University: Department of Statistical Science\n\n\n\n",
    "preview": "posts/2021-12-10-poker-hands-probabilities/Poker_Hands_1200x754.jpg",
    "last_modified": "2021-12-12T20:26:35+11:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-11-25-top-100-baby-names/",
    "title": "Top 100 Baby Names",
    "description": "Top 100 Baby Names registered in NSW from 1952 to 2019",
    "author": [
      {
        "name": "Mike Hudson",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2021-11-25",
    "categories": [
      "r project"
    ],
    "contents": "\n\n\n\nA project I undertook to practice my data wrangling skills. NSW BDM provides the raw data in pdf format, so the biggest challenge was extracting that data into something I could work with.\nI have the data for 2020. Just need some spare time to update the project.\nYou can see the racing bar charts here:\nBoys names\nGirls names\n\n\n\n",
    "preview": "posts/2021-11-25-top-100-baby-names/top-100-baby-names.png",
    "last_modified": "2021-11-26T15:16:11+11:00",
    "input_file": {},
    "preview_width": 1528,
    "preview_height": 736
  }
]
